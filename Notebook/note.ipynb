{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding: Tutoriel 01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn #For load model\n",
    "import torch.nn.functional as F #For having acces to all functions available in pytorch hub\n",
    "import torch.optim as optim  #For optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3 #Given 2 word we should predict the next word \n",
    "EMBEDDING_DIM = 10 #For represent each word inside the text in to a 10 dimentional space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Docteur : Bonjour, comment ça va aujourd'hui ?\n",
    "\n",
    "Patient : Bonjour, j'ai des douleurs dans le bas-ventre depuis quelques semaines maintenant et je suis très inquiète.\n",
    "\n",
    "Docteur : Je vois. Pouvez-vous me dire où exactement vous ressentez ces douleurs ?\n",
    "\n",
    "Patient : C'est principalement dans la région ovarienne, mais ça peut aussi irradier dans le dos.\n",
    "\n",
    "Docteur : D'accord. Est-ce que vous avez remarqué d'autres symptômes comme des saignements anormaux ou des pertes vaginales inhabituelles ?\n",
    "\n",
    "Patient : Oui, j'ai remarqué que mes règles étaient plus abondantes et plus longues que d'habitude. Et j'ai aussi des pertes vaginales un peu plus abondantes.\n",
    "\n",
    "Docteur : Je comprends. Avez-vous remarqué si ces symptômes sont liés à votre cycle menstruel ou s'ils se produisent de manière aléatoire ?\n",
    "\n",
    "Patient : C'est difficile à dire, mais je pense que ça se produit de manière aléatoire.\n",
    "\n",
    "Docteur : Très bien. Nous allons devoir faire des examens pour en savoir plus sur ce qui se passe. Il est possible que nous ayons besoin de faire une échographie ou une biopsie pour mieux comprendre la situation.\n",
    "\n",
    "Patient : D'accord, je comprends. Mais ça ne peut pas être quelque chose de grave, n'est-ce pas ?\n",
    "\n",
    "Docteur : Il est difficile de dire pour le moment. Nous devons d'abord faire des examens pour en savoir plus. Mais ne vous inquiétez pas, nous allons tout faire pour trouver la cause de vos symptômes et vous aider à vous sentir mieux.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"Patient :\" , \"\")\n",
    "text = text.replace(\"Docteur :\" , \"\")\n",
    "text = text.strip()\n",
    "sliceText = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-GRAM\n",
    "\n",
    "It is a nlp technique used to slice a text in N succession of word. \n",
    "eg : text = papa est sorti voir maman au village \n",
    "2 grams:\n",
    "papa est \n",
    "est sorti\n",
    "sorti voir \n",
    "voir maman\n",
    "maman au \n",
    "au village"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reversing a list using slicing technique\n",
    "def Reverse(lst):\n",
    "    new_lst = lst[::-1]\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [\n",
    "    (\n",
    "        Reverse([sliceText[i - j - 1] for j in range(CONTEXT_SIZE)]),\n",
    "        sliceText[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(sliceText))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(sliceText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = {word : idx for idx , word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1060.3521480560303, 1054.8881106376648, 1049.4772410392761, 1044.1156001091003, 1038.7993278503418, 1033.5264983177185, 1028.288739681244, 1023.0860347747803, 1017.9147090911865, 1012.7678003311157]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([ids[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([ids[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2569, -0.3631, -0.0493, -0.5748,  0.0583,  0.2807, -0.4966, -2.1175,\n",
      "         0.6554,  0.2405], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[ids[\"remarqué\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('lnitvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7da94f4bbe617fa706b25166a79bd1a71d6302505372ab0c4e237403f107a285"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
